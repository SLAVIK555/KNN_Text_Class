{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "curious-publication",
   "metadata": {},
   "source": [
    "Классификация текстов при помощи метода K ближайших соседей\n",
    "\n",
    "Постановка задачи: разработать программу для классификации текстов при помощи метода K ближайших соседей\n",
    "Для этого нам нужно:\n",
    "- Разработать функционал извлечения текстовых данных из файлов, отбрасывая стоп-слова для русского языка (стоп-слова - это слова, которые довольно часто встречаются в разговорной речи и письме, но при этом не несут никакой смысловой нагрузки, а следовательно не могут быть маркерами для текстов определенного класса)\n",
    "- Разработать функционал для составления списка наиболее часто встречающихся слов для каждого класса текстов\n",
    "- Разработать функционал для составления нормализованного вектора для каждого текста и его дальнейшего включения в датасет\n",
    "- Разработать функционал для обеспечения функционирования непосредственно метода K ближайших соседей (вычисление эйлерова расстояния, нажоджения всех ближайших соседей и т.д.)\n",
    "- Разработать функционал для тестирования полученной нейросети на незнакомом ей тексте\n",
    "\n",
    "Итак приступим\n",
    "\n",
    "Начнем с датасета. В класс \"Детектив\" вошли отрывки из произведений Акрура Конан-Дойля, Агаты Кристи, Эдгара По и других писателей детективистов, а в класс \"Космическая фантастика\" вошли отрывки из произведений Айзека Азимова, Энди Уира, Лоиса Буджолда и других писателей фантастов. Исходные данные представлены в папке \"Dataset/Detective\" и \"Dataset/Space fiction\"\n",
    "\n",
    "А теперь приступим непосредственно к коду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "animal-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем все необходимое\n",
    "import os\n",
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac9c12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Your_path = \"/home/slava/Source\"#Можете поменять это для себя\n",
    "Dir = \"/KNN_Text_Class\"\n",
    "\n",
    "source_path = Your_path + Dir + \"/Dataset\"\n",
    "stoppath = Your_path + Dir + \"/stopwords.txt\"\n",
    "\n",
    "vector_path = Your_path + Dir + \"/Vector.txt\"\n",
    "new_vector_path = Your_path + Dir + \"/NewVector.txt\"\n",
    "\n",
    "test_path = Your_path + Dir + \"/Dataset/Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258eddf",
   "metadata": {},
   "source": [
    "Добавим функционал для подсчета частоты встречи слов в исходном датасете, чтобы определить наиболее часто встречающиеся слова, которые войдут в вектор для дальнейшей классификации. При этом при подстчете слов используется так называемый стоплист - список слов для русского языка, которые довольно часто встречаются в разговорной речи и письме, но при этом не несут никакой смысловой нагрузки, а следовательно не могут быть маркерами для текстов определенного класса. Примеры таких слов приведены в файле stopwords.txt. Эти слова в общем подсчете не участвуют."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94d8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_norm(str1):\n",
    "    string = str(str1.lower())\n",
    "\n",
    "    if string.find('.'):\n",
    "        string = string.replace('.', '')\n",
    "\n",
    "    if string.find(','):\n",
    "        string = string.replace(',', '')\n",
    "\n",
    "    if string.find(':'):\n",
    "        string = string.replace(':', '')\n",
    "\n",
    "    if string.find(';'):\n",
    "        string = string.replace(';', '')\n",
    "\n",
    "    if string.find('-'):\n",
    "        string = string.replace('-', '')\n",
    "\n",
    "    if string.find('_'):\n",
    "        string = string.replace('_', '')\n",
    "\n",
    "    if string.find('!'):\n",
    "        string = string.replace('!', '')\n",
    "\n",
    "    if string.find('?'):\n",
    "        string = string.replace('?', '')\n",
    "\n",
    "    return string\n",
    "\n",
    "def word_freq_counter(source_path, stoppath, word):\n",
    "    filelist = []\n",
    "    for root, dirs, files in os.walk(source_path + \"/\" + word): \n",
    "        for file in files: \n",
    "            #append the file name to the list \n",
    "            filelist.append(file)\n",
    "\n",
    "    #print all the file names \n",
    "    for name in filelist: \n",
    "        print(name)\n",
    "\n",
    "    with open(stoppath) as stopfile:\n",
    "        stoplist = stopfile.read().split()\n",
    "    #print (stoplist)\n",
    "\n",
    "    main_word_filelist = []\n",
    "    main_freq_filelist = []\n",
    "\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    for name in filelist: \n",
    "        print(name)\n",
    "        #input()\n",
    "\n",
    "        with open(source_path + \"/\" + word + \"/\" + name) as inputfile:\n",
    "            list_data = inputfile.read().split() # читаем с файла разбиваем по пробелам\n",
    "            #print(list_data)\n",
    "            for item in list_data:\n",
    "                litem = string_norm(item)\n",
    "                if litem not in stoplist:\n",
    "                    #print(litem)\n",
    "                    #input()\n",
    "                    if litem in main_word_filelist:\n",
    "                        litem_index = main_word_filelist.index(litem)\n",
    "                        litem_freq = main_freq_filelist[litem_index]\n",
    "                        new_litem_freq = litem_freq + 1\n",
    "                        main_freq_filelist[litem_index] = new_litem_freq\n",
    "                    else:\n",
    "                        main_word_filelist.append(litem)\n",
    "                        main_freq_filelist.append(1)\n",
    "\n",
    "    outfile = open(word + \".txt\", 'w')\n",
    "\n",
    "    for i in range(2000):\n",
    "        max_item_freq = max(main_freq_filelist)\n",
    "        max_index = main_freq_filelist.index(max_item_freq)\n",
    "        max_item_word = main_word_filelist[max_index]\n",
    "\n",
    "        outstring = str(max_item_freq) + \": \" + str(max_item_word) + \"\\n\"\n",
    "\n",
    "        outfile.write(outstring)\n",
    "\n",
    "        main_word_filelist.pop(max_index)\n",
    "        main_freq_filelist.pop(max_index)\n",
    "\n",
    "    outfile.close()\n",
    "    \n",
    "def wfc_main(word):\n",
    "    source_path = \"/home/slava/Source/KNN_Text_Class/Dataset\"\n",
    "    stoppath = \"/home/slava/Source/KNN_Text_Class/stopwords.txt\"\n",
    "    #word = \"Detective\"\n",
    "    #word = \"Space_fiction\"\n",
    "\n",
    "    word_freq_counter(source_path, stoppath, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e23041e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "12\n",
      "11\n",
      "2\n",
      "18\n",
      "6\n",
      "16\n",
      "7\n",
      "4\n",
      "15\n",
      "19\n",
      "20\n",
      "17\n",
      "10\n",
      "3\n",
      "5\n",
      "13\n",
      "1\n",
      "9\n",
      "14\n",
      "-----------------\n",
      "8\n",
      "12\n",
      "11\n",
      "2\n",
      "18\n",
      "6\n",
      "16\n",
      "7\n",
      "4\n",
      "15\n",
      "19\n",
      "20\n",
      "17\n",
      "10\n",
      "3\n",
      "5\n",
      "13\n",
      "1\n",
      "9\n",
      "14\n",
      "8\n",
      "12\n",
      "11\n",
      "2\n",
      "18\n",
      "6\n",
      "16\n",
      "7\n",
      "4\n",
      "15\n",
      "19\n",
      "20\n",
      "17\n",
      "10\n",
      "3\n",
      "5\n",
      "13\n",
      "1\n",
      "9\n",
      "14\n",
      "-----------------\n",
      "8\n",
      "12\n",
      "11\n",
      "2\n",
      "18\n",
      "6\n",
      "16\n",
      "7\n",
      "4\n",
      "15\n",
      "19\n",
      "20\n",
      "17\n",
      "10\n",
      "3\n",
      "5\n",
      "13\n",
      "1\n",
      "9\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# \"Detective\"\n",
    "# \"Space_fiction\"\n",
    "\n",
    "wfc_main(\"Detective\")\n",
    "wfc_main(\"Space_fiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083bc60",
   "metadata": {},
   "source": [
    "После того, как мы определили наиболее часто встречающиеся слова для классов Детектив и Космическая фантастика (файлы Detective.txt и Space_fiction.txt), нам предстоит на наше усмотрение выбрать те слова, которые встречаются достаточно часто и по нашему мнению отлично характеризуют данный класс текстов, после чего внести эти слова в наш вектор (файл Vector.txt), а после чего удалить повторяющиеся слова (файл NewVector.txt). Реализуем этот функционал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e0a519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_deleter(vector_path, new_vector_path):\n",
    "    with open(vector_path) as vectorfile:\n",
    "        vectorlist = vectorfile.read().split()\n",
    "\n",
    "        rd_vectorlist = list(set(vectorlist))\n",
    "\n",
    "        ofile = open(new_vector_path, 'w')\n",
    "        for item in rd_vectorlist:\n",
    "            ofile.write(item + \"\\n\")\n",
    "\n",
    "        ofile.close()\n",
    "        \n",
    "repeat_deleter(vector_path, new_vector_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac11785",
   "metadata": {},
   "source": [
    "Далее перейдем к составлению датасета в виде массива для нашего классификатора\n",
    "\n",
    "Реализуем функционал для составления датасета в виде набора векторов из исходного набора текстов. При этом каждому тексту сопоставляется вектор, который содержит частоты появления каждого из слов, находящихся в нашем контрольном списке слов (файл NewVector.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b5f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_vector(vector):\n",
    "    norm_vector = [0] * len(vector)#vectorsize\n",
    "    minv = min(vector)\n",
    "    maxv = max(vector)\n",
    "    delta = maxv-minv\n",
    "\n",
    "    for i in range(len(norm_vector)):\n",
    "        norm_vector[i] = (vector[i]-minv)/delta\n",
    "\n",
    "    return norm_vector\n",
    "\n",
    "def fit_func(full_path_to_text, vectorlist):\n",
    "    vector = [0] * len(vectorlist)\n",
    "\n",
    "    with open(full_path_to_text) as inputfile:\n",
    "        list_data = inputfile.read().split() # читаем с файла разбиваем по пробелам\n",
    "        for item in list_data:\n",
    "            litem = string_norm(item)\n",
    "            if litem in vectorlist:\n",
    "                index = vectorlist.index(litem)\n",
    "                v_item = vector[index]\n",
    "                n_v_item = v_item + 1\n",
    "                vector[index] = n_v_item\n",
    "\n",
    "    norm_vector = normalization_vector(vector)\n",
    "    \n",
    "    return norm_vector\n",
    "\n",
    "def dataset_maker(new_vector_path, source_path, word, dataset, number):\n",
    "    with open(new_vector_path) as vectorfile:\n",
    "        vectorlist = vectorfile.read().split()\n",
    "\n",
    "    filelist = []\n",
    "    for root, dirs, files in os.walk(source_path + \"/\" + word): \n",
    "        for file in files: \n",
    "            #append the file name to the list \n",
    "            filelist.append(file)\n",
    "\n",
    "    #print all the file names \n",
    "    for name in filelist: \n",
    "        print(name)\n",
    "        \n",
    "        norm_vector = fit_func(source_path + \"/\" + word + \"/\" + name, vectorlist)\n",
    "\n",
    "        norm_vector.append(number)\n",
    "\n",
    "        dataset.append(norm_vector)\n",
    "        # print(norm_vector)\n",
    "        # input()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def dm_main():\n",
    "    main_dataset = []\n",
    "\n",
    "    det = dataset_maker(new_vector_path, source_path, \"Detective\", main_dataset, 0)\n",
    "    print(\"----\")\n",
    "    sf = dataset_maker(new_vector_path, source_path, \"Space_fiction\", main_dataset, 1)\n",
    "\n",
    "    #main_dataset = [det, sf]#det is 0, sf is 1\n",
    "    print(len(main_dataset[0]))\n",
    "\n",
    "    np.save(\"DSDataset.npy\", main_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8603423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "12\n",
      "11\n",
      "2\n",
      "18\n",
      "6\n",
      "16\n",
      "7\n",
      "4\n",
      "15\n",
      "19\n",
      "20\n",
      "17\n",
      "10\n",
      "3\n",
      "5\n",
      "13\n",
      "1\n",
      "9\n",
      "14\n",
      "----\n",
      "8\n",
      "12\n",
      "11\n",
      "2\n",
      "18\n",
      "6\n",
      "16\n",
      "7\n",
      "4\n",
      "15\n",
      "19\n",
      "20\n",
      "17\n",
      "10\n",
      "3\n",
      "5\n",
      "13\n",
      "1\n",
      "9\n",
      "14\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "dm_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246db800",
   "metadata": {},
   "source": [
    "Реализуем функционал для классификации нового текста на основе составленного датасета. При классификации нового текста данный текст прогоняется черех фит-функцию (для него находится вектор частот появления контрольных слов как при составлении датасета из исходных текстов), после чего рассчитывается евклидово расстояние между вектором нового текста и всеми векторами всех текстов, представленных в датасете. Затем определяется принадлежность к классам K ближайших векторов. Класс нового вектора считается равным классу тех векторов, которых больше в выборке из K ближайших векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb1d3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "\n",
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors \n",
    "\n",
    "# Make a classification prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction, output_values\n",
    "\n",
    "def text_fit(full_path_to_text, full_path_to_vector):\n",
    "    with open(full_path_to_vector) as inputvec:\n",
    "        vectorlist = inputvec.read().split() # читаем с файла разбиваем по пробелам\n",
    "        \n",
    "    norm_vector = fit_func(full_path_to_text, vectorlist)\n",
    "\n",
    "    return norm_vector\n",
    "\n",
    "def p_main():\n",
    "    dataset = np.load(\"DSDataset.npy\")\n",
    "\n",
    "    fitd1 = text_fit(test_path + \"/\" + \"D1\", new_vector_path)\n",
    "    fitd2 = text_fit(test_path + \"/\" + \"D2\", new_vector_path)\n",
    "    fits1 = text_fit(test_path + \"/\" + \"S1\", new_vector_path)\n",
    "    fits2 = text_fit(test_path + \"/\" + \"S2\", new_vector_path)\n",
    "\n",
    "    print(\"0 is Detective, 1 is Space Fiction\")\n",
    "    print(\"----------------------------------\")\n",
    "    print(\" \")\n",
    "\n",
    "    print(\"Test text is D1\")\n",
    "    prediction, list_data = predict_classification(dataset, fitd1, 3)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (0, prediction, 3, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fitd1, 5)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (0, prediction, 5, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fitd1, 7)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (0, prediction, 7, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fitd1, 9)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (0, prediction, 9, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\" \")\n",
    "\n",
    "    print(\"Test text is D2\")\n",
    "    prediction, list_data = predict_classification(dataset, fitd2, 3)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (0, prediction, 3, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fitd2, 5)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (0, prediction, 5, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fitd2, 7)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (0, prediction, 7, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fitd2, 9)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (0, prediction, 9, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\" \")\n",
    "\n",
    "    print(\"Test text is S1\")\n",
    "    prediction, list_data = predict_classification(dataset, fits1, 3)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (1, prediction, 3, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fits1, 5)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (1, prediction, 5, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fits1, 7)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (1, prediction, 7, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fits1, 9)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (1, prediction, 9, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\" \")\n",
    "\n",
    "    print(\"Test text is S2\")\n",
    "    prediction, list_data = predict_classification(dataset, fits2, 3)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (1, prediction, 3, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fits2, 5)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (1, prediction, 5, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fits2, 7)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (1, prediction, 7, list_data))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    prediction, list_data = predict_classification(dataset, fits2, 9)\n",
    "    print('Expected %d, Got %d, K %d, List %s' % (1, prediction, 9, list_data))\n",
    "    print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ad7291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is Detective, 1 is Space Fiction\n",
      "----------------------------------\n",
      " \n",
      "Test text is D1\n",
      "Expected 0, Got 0, K 3, List [0.0, 0.0, 0.0]\n",
      "----------------------------------------------------\n",
      "Expected 0, Got 0, K 5, List [0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "----------------------------------------------------\n",
      "Expected 0, Got 0, K 7, List [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n",
      "----------------------------------------------------\n",
      "Expected 0, Got 0, K 9, List [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]\n",
      "----------------------------------------------------\n",
      " \n",
      "Test text is D2\n",
      "Expected 0, Got 0, K 3, List [0.0, 0.0, 0.0]\n",
      "----------------------------------------------------\n",
      "Expected 0, Got 0, K 5, List [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "----------------------------------------------------\n",
      "Expected 0, Got 0, K 7, List [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
      "----------------------------------------------------\n",
      "Expected 0, Got 0, K 9, List [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]\n",
      "----------------------------------------------------\n",
      " \n",
      "Test text is S1\n",
      "Expected 1, Got 1, K 3, List [1.0, 1.0, 0.0]\n",
      "----------------------------------------------------\n",
      "Expected 1, Got 0, K 5, List [1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "----------------------------------------------------\n",
      "Expected 1, Got 1, K 7, List [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
      "----------------------------------------------------\n",
      "Expected 1, Got 1, K 9, List [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "----------------------------------------------------\n",
      " \n",
      "Test text is S2\n",
      "Expected 1, Got 1, K 3, List [0.0, 1.0, 1.0]\n",
      "----------------------------------------------------\n",
      "Expected 1, Got 1, K 5, List [0.0, 1.0, 1.0, 0.0, 1.0]\n",
      "----------------------------------------------------\n",
      "Expected 1, Got 1, K 7, List [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]\n",
      "----------------------------------------------------\n",
      "Expected 1, Got 1, K 9, List [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "p_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274d08b",
   "metadata": {},
   "source": [
    "Расшифруем вывод:\n",
    "В первой строке дается обозначение каждого из классов в цифровом виде.\n",
    "Далее расшифруем вывод на примере одной из строки, в частности:\n",
    "\n",
    "Expected 0, Got 0, K 5, List [0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "\n",
    "Expected 0 - ожидалось получить на вход текст из класса 0\n",
    "\n",
    "Got 0 - текст классифицирован как текст класса 0\n",
    "\n",
    "K 5 - текст классифицировали по 5 ближайшим соседям\n",
    "\n",
    "List [0.0, 0.0, 0.0, 0.0, 1.0] - Список ближайших соседей, среди ближайших соседей 4 текста класса 0 и 1 текст класса 1\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "Еще одна строка:\n",
    "\n",
    "Expected 1, Got 0, K 5, List [1.0, 1.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "Expected 1 - ожидалось получить на вход текст из класса 1\n",
    "\n",
    "Got 0 - текст классифицирован как текст класса 0, мы потерпели неудачу :(\n",
    "\n",
    "K 5 - текст классифицировали по 5 ближайшим соседям\n",
    "\n",
    "List [1.0, 1.0, 0.0, 0.0, 0.0] - Список ближайших соседей, среди ближайших соседей 3 текста класса 0 и 2 текст класса 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe023b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
